# coding: utf-8
import gc
import time

import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import KFold
from sklearn import metrics




filename = 'factored.pkl'

train_fname_f = 'train-' + filename
test_fname_f = 'test-' + filename

train_fname = '../input/{}'.format(train_fname_f)
test_fname = '../input/{}'.format(test_fname_f)

train = pd.read_pickle(train_fname)
print("TRAIN LOADED")
test = pd.read_pickle(test_fname)
print("TEST LOADED")

# ### 2.2 Define the type of each variable

true_numerical_columns = [
    'Census_ProcessorCoreCount',
    'Census_PrimaryDiskTotalCapacity',
    'Census_SystemVolumeTotalCapacity',
    'Census_TotalPhysicalRAM',
    'Census_InternalPrimaryDiagonalDisplaySizeInInches',
    'Census_InternalPrimaryDisplayResolutionHorizontal',
    'Census_InternalPrimaryDisplayResolutionVertical',
    'Census_InternalBatteryNumberOfCharges'
]

# We also list binary variables, since they can be treated as numericals
# by tree methods:
binary_variables = [c for c in train.columns if train[c].nunique() == 2]
# to finally make a census of the categorical variables:
categorical_columns = [c for c in train.columns
                       if (c not in true_numerical_columns) & (c not in binary_variables)]

target = train['HasDetections']
del train['HasDetections']

param_quick = {'num_leaves': 60,
               'min_data_in_leaf': 60,
               'objective': 'binary',
               'max_depth': -1,
               'learning_rate': 0.1,
               "boosting": "gbdt",
               "feature_fraction": 0.8,
               "bagging_freq": 1,
               "bagging_fraction": 0.8,
               "bagging_seed": 11,
               "metric": 'binary_logloss',
               "lambda_l1": 0.1,
               "random_state": 133,
               "verbosity": -1}

param = {
    "boosting": "gbdt", 
    "objective": "binary",
    "metric": "auc", 
    "learning_rate": 0.05, 
    "max_depth": 5,
    "num_leaves": 40,
    "feature_fraction": 0.7, 
    "bagging_fraction": 0.7, 
    "bagging_freq": 1,
    "lambda_l1": 0.1, 
    "lambda_l2": 0.1
}               

max_iter = 5

# for fraction, min_data, num_round in ((0.7, 2048, 10000), (0.6, 1024,
# 10000), (0.7, 1024, 5000), (0.75, 2048, 10000), (0.6, 1024, 5000)):

param_max = {'num_leaves': 512,
         'max_bin': 1024,
         'min_data_in_leaf': 2048,
         'objective': 'binary',
         'max_depth': -1,
         'learning_rate': 0.1,
         "boosting": "dart",
         "feature_fraction": 0.7,
         "bagging_freq": 1,
         "bagging_fraction": 0.7,
         "bagging_seed": 11,
         "metric": 'binary_logloss',
         "lambda_l1": 0.1,
         "random_state": 133,
         "verbosity": -1}

gc.collect()

folds = KFold(n_splits=5, shuffle=True, random_state=15)
oof = np.zeros(len(train))
categorical_columns = [
    c for c in categorical_columns if c not in ['MachineIdentifier']]
features = [c for c in train.columns if c not in ['MachineIdentifier']]
predictions = np.zeros(len(test))
start = time.time()
feature_importance_df = pd.DataFrame()
start_time = time.time()
score = [0 for _ in range(folds.n_splits)]

print("STARTING K-FOLD CV")
for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):
    print("fold nÂ°{}".format(fold_))
    trn_data = lgb.Dataset(train.iloc[trn_idx][features],
                           label=target.iloc[trn_idx],
                           categorical_feature=categorical_columns
                           )
    val_data = lgb.Dataset(train.iloc[val_idx][features],
                           label=target.iloc[val_idx],
                           categorical_feature=categorical_columns
                           )

    num_round = 10000
    clf = lgb.train(param,
                    trn_data,
                    num_round,
                    valid_sets=[trn_data, val_data],
                    verbose_eval=100,
                    early_stopping_rounds=200)

    oof[val_idx] = clf.predict(
        train.iloc[val_idx][features], num_iteration=clf.best_iteration)

    fold_importance_df = pd.DataFrame()
    fold_importance_df["feature"] = features
    fold_importance_df["importance"] = clf.feature_importance(
        importance_type='gain')
    fold_importance_df["fold"] = fold_ + 1
    feature_importance_df = pd.concat(
        [feature_importance_df, fold_importance_df], axis=0)

    # we perform predictions by chunks
    initial_idx = 0
    chunk_size = 1000000
    current_pred = np.zeros(len(test))
    while initial_idx < test.shape[0]:
        final_idx = min(initial_idx + chunk_size, test.shape[0])
        idx = range(initial_idx, final_idx)
        current_pred[idx] = clf.predict(
            test.iloc[idx][features], num_iteration=clf.best_iteration)
        initial_idx = final_idx
    predictions += current_pred / min(folds.n_splits, max_iter)

    #predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / min(folds.n_splits, max_iter)

    print("time elapsed: {:<5.2} min".format(
        (time.time() - start_time) / 60))
    score[fold_] = metrics.roc_auc_score(
        target.iloc[val_idx], oof[val_idx])
    if fold_ == max_iter - 1:
        break

if (folds.n_splits == max_iter):
    cv_score = metrics.roc_auc_score(target, oof)
else:
    cv_score = sum(score) / max_iter

cv_score_printable = "{:<8.5f}".format(cv_score)
cv_score_printable = cv_score_printable.strip()

print("CV score: {}".format(cv_score_printable))
print("Total time elapsed: {:<5.2} min".format((time.time() - start) / 60))

sub_df = pd.DataFrame(
    {"MachineIdentifier": test["MachineIdentifier"].values})
sub_df["HasDetections"] = predictions
sub_df[:10]

#sub_df.to_csv("../output/submit_dart_bigtree.csv", index=False)

import kaggle
import os

model_dir = '../output'
cv_score_printable = cv_score_printable.replace(".", "")

model_name = 'submit_{}_nl{}_mdl{}_cv{}.csv.gz'.format(param['boosting'], param[
    'num_leaves'], param['min_data_in_leaf'], cv_score_printable)

fname = os.path.join(model_dir, model_name)
param_string = ', '.join(('{}: {}'.format(k, v) for k, v in param.items()))
message = 'DATA: {} LGBM params: {}'.format(train_fname, param_string)
competition = 'microsoft-malware-prediction'

sub_df.to_csv(fname, compression='gzip', index=False)
kaggle.api.competition_submit(os.path.abspath(fname), message, competition)
