# coding: utf-8
import os
import time
import gc


import kaggle
import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn import metrics
import matplotlib.pyplot as plt
import seaborn as sns


train = pd.read_pickle('../input/train.pkl')
print("TRAIN LOADED")
test = pd.read_pickle('../input/test.pkl')
print("TEST LOADED")


experiment_nr = 51
random_state = 42


experinment_nr = 49 
train_fname = '../input/train-encoded-full.pkl'
test_fname = '../input/test-encoded-full.pkl'

train = pd.read_pickle(train_fname)
print("TRAIN LOADED")


target = pd.read_pickle('../input/target.pkl')




gc.collect()

print("TRAIN PREPARED")


test = pd.read_pickle(test_fname)
print("TEST LOADED")

true_numerical_columns = [
    'Census_ProcessorCoreCount',
    'Census_PrimaryDiskTotalCapacity',
    'Census_SystemVolumeTotalCapacity',
    'Census_TotalPhysicalRAM',
    'Census_InternalPrimaryDiagonalDisplaySizeInInches',
    'Census_InternalPrimaryDisplayResolutionHorizontal',
    'Census_InternalPrimaryDisplayResolutionVertical',
    'Census_InternalBatteryNumberOfCharges',
    'fe_avsig_gamer_freq',
    'fe_cpucores_region_freq', 
    'fe_cpucores_oemname_freq',
    'fe_geoname_oemname_freq', 
    'fe_non_primary_drive_MB',
    'fe_screen_area'
]


# We also list binary variables, since they can be treated as numericals
# by tree methods:
binary_variables = [c for c in train.columns if train[c].nunique() == 2]
# to finally make a census of the categorical variables:
categorical_columns = [c for c in train.columns
                       if (c not in true_numerical_columns) & (c not in binary_variables)]

gc.collect()



categorical_columns = [
    c for c in categorical_columns if c not in ['MachineIdentifier']]

features = [c for c in train.columns if c not in ['MachineIdentifier']]
predictions = np.zeros(len(test))
start = time.time()
feature_importance_df = pd.DataFrame()
start_time = time.time()

print("STARTING TRAINING")


lgb_params = {
    'num_leaves': 60,
    'min_data_in_leaf': 60,
    'max_bin': 4096,
    "boosting": "gbdt",
    'objective': 'binary',
    "metric": 'auc',
    'max_depth': -1,
    'learning_rate': 0.1,
    "feature_fraction": 0.8,
    "bagging_freq": 1,
    "bagging_fraction": 0.8,
    "bagging_seed": 11,
    "lambda_l1": 0.1,
    "random_state": 133,
    "verbosity": -1
}


x_train, x_val, y_train, y_val = train_test_split(train, target, test_size=0.15, random_state=random_state)

del train
gc.collect()

trn_data = lgb.Dataset(x_train[features],
                       label=y_train,
                       categorical_feature=categorical_columns
                       )
val_data = lgb.Dataset(x_val[features],
                       label=y_val,
                       categorical_feature=categorical_columns
                       )

num_round = 10000
clf = lgb.train(lgb_params,
                trn_data,
                num_round,
                valid_sets=[trn_data, val_data],
                verbose_eval=100,
                early_stopping_rounds=50)

oof = clf.predict(x_val[features], num_iteration=clf.best_iteration)

fold_importance_df = pd.DataFrame()
fold_importance_df["feature"] = features
fold_importance_df["importance"] = clf.feature_importance(
    importance_type='gain')
feature_importance_df = pd.concat(
    [feature_importance_df, fold_importance_df], axis=0)


test = pd.read_pickle(test_fname)
print("TEST LOADED")

# we perform predictions by chunks
initial_idx = 0
chunk_size = 1000000
predictions = np.zeros(len(test))
while initial_idx < test.shape[0]:
    final_idx = min(initial_idx + chunk_size, test.shape[0])
    idx = range(initial_idx, final_idx)
    predictions[idx] = clf.predict(
        test.iloc[idx][features], num_iteration=clf.best_iteration)
    initial_idx = final_idx

print("time elapsed: {:<5.2}s".format((time.time() - start_time) / 3600))
score = metrics.roc_auc_score(y_val, oof)


cv_score = metrics.roc_auc_score(target, oof)

cv_score_printable = "{:<8.5f}".format(cv_score)
print("CV score: {}".format(cv_score_printable))

cv_score_printable = cv_score_printable.replace(".", "")
cv_score_printable = cv_score_printable.strip()



# Feature importance
cols = (feature_importance_df[["feature", "importance"]]
        .groupby("feature")
        .mean()
        .sort_values(by="importance", ascending=False)[:1000].index)

best_features = feature_importance_df.loc[
    feature_importance_df.feature.isin(cols)]

plt.figure(figsize=(14, 25))
sns.barplot(x="importance",
            y="feature",
            data=best_features.sort_values(by="importance",
                                           ascending=False))
plt.title('LightGBM Features (avg over folds)')
plt.tight_layout()
plt.savefig('../img/e{}_lgbm_importances_{}.png'.format(experinment_nr, cv_score_printable))
feature_importance_df.to_csv('../EDA/e{}_lgbm_importances_{}.csv'.format(experinment_nr, cv_score_printable))


# submit predictions

sub_df = pd.read_csv('../input/sample_submission.csv')
sub_df["HasDetections"] = predictions

model_dir = '../output'


model_name = 'submit_e{}_cv{}.csv.gz'.format(experinment_nr, cv_score_printable)

fname = os.path.join(model_dir, model_name)
param_string = ', '.join(('{}: {}'.format(k, v) for k, v in lgb_params.items()))
message = 'CV: {} DATA: {} LGBM params: {}'.format(cv_score_printable, train_fname, param_string)
competition = 'microsoft-malware-prediction'

sub_df.to_csv(fname, compression='gzip', index=False)
kaggle.api.competition_submit(os.path.abspath(fname), message, competition)
