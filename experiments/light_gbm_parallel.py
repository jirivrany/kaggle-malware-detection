# coding: utf-8
import warnings
import gc
import pickle
import time
import kaggle
import os


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import lightgbm as lgb
from sklearn.model_selection import KFold

from sklearn import metrics


from multiprocessing import Lock, Process, Queue, current_process

"""
E61
- parallel CV
"""
print("script started: ", time.strftime("%b %d %Y %H:%M:%S"))


experinment_nr = 61
#train_fname = '../input/train-encoded-full-corr-v2.pkl'
#test_fname = '../input/test-encoded-full-corr-v2.pkl'

train_fname = '../input/train_catboost.pkl.gz'
test_fname = '../input/test_catboost.pkl.gz'

TRAIN = pd.read_pickle(train_fname)
print("TRAIN LOADED")


TARGET = pd.read_pickle('../input/target.pkl')


gc.collect()
print("TRAIN PREPARED")
TEST = pd.read_pickle(test_fname)
print("TEST LOADED")

folds_nr = 3

folds = KFold(n_splits=folds_nr, shuffle=True, random_state=15)
oof = np.zeros(len(TRAIN))
features = [c for c in TRAIN.columns if c not in ['MachineIdentifier']]
predictions = np.zeros(len(TEST))
feature_importance_df = pd.DataFrame()
score = [0 for _ in range(folds.n_splits)]
work_queue = Queue()
done_queue = Queue()
processes = []


PARAM = {
    'num_threads': 9,
    'num_leaves': 60,
    'min_data_in_leaf': 60,
    "boosting": "gbdt",
    'objective': 'binary',
    "metric": 'auc',
    'max_depth': -1,
    'learning_rate': 0.2,
    "feature_fraction": 0.8,
    "bagging_freq": 1,
    "bagging_fraction": 0.8,
    "bagging_seed": 11,
    "lambda_l1": 0.1,
    "random_state": 133,
    "verbosity": -1
}


def worker(work_queue, done_queue):
    """
    Worker si bere data z fronty, dokud tam nějaká jsou a volá na ně procesní funkci
    """

    try:
        for data in iter(work_queue.get, 'STOP'):

            trn_idx = data['trn_idx']
            val_idx = data['val_idx']
            features = data['features']

            # create dataset for lightgbm
            trn_data = lgb.Dataset(TRAIN.iloc[trn_idx][features],
                                   label=TARGET.iloc[trn_idx],
                                   )
            val_data = lgb.Dataset(TRAIN.iloc[val_idx][features],
                                   label=TARGET.iloc[val_idx],
                                   )
            # create classifier
            clf = lgb.train(PARAM,
                            trn_data,
                            data['num_round'],
                            valid_sets=[trn_data, val_data],
                            verbose_eval=100,
                            early_stopping_rounds=200)

            # predict
            oof_fo = clf.predict(
                    TRAIN.iloc[val_idx][features], num_iteration=clf.best_iteration)

            result = {
                'fold': data['fold'],
                'val_idx': val_idx,
                'trn_idx': trn_idx,
                'oof': oof_fo,
                'clf': clf,
                'best_iteration': clf.best_iteration,
                'importance': clf.feature_importance(importance_type='gain')
            }

            done_queue.put(result)

    except Exception as e:
        done_queue.put("{} failed on {} with: {}".format(
            current_process().name, data['PARAM'], e))
    return True


start = time.time()
print("STARTING PARALLEL K-FOLD CV", time.strftime("%b %d %Y %H:%M:%S"))
for fold_, (trn_idx, val_idx) in enumerate(folds.split(TRAIN.values, TARGET.values)):
    print("fold n°{}".format(fold_))

    num_round = 5200

    data = {
        'val_idx': val_idx,
        'trn_idx': trn_idx,
        'fold': fold_,
        'features': features,
    }
    work_queue.put(data)


for w in range(4):
    # nastartujeme nový process - worker musí dostat frontu jako argument
    p = Process(target=worker, args=(work_queue, done_queue))
    p.start()
    processes.append(p)
    print("started process", w)
    # STOP příkaz musíme přidat pro každý process který jsme vytvořili
    work_queue.put('STOP')


for p in processes:
    p.join()

done_queue.put('STOP')

print("training completed: ", time.strftime("%b %d %Y %H:%M:%S"))

print("training time", time.time() - start)

print("starting prediction: ", time.strftime("%b %d %Y %H:%M:%S"))
start = time.time()

for result in iter(done_queue.get, 'STOP'):
    val_idx = result['val_idx']
    fold_nr = result['fold']

    oof[val_idx] = result['oof']
    clf = result['clf']

    fold_importance_df = pd.DataFrame()
    fold_importance_df["feature"] = features
    fold_importance_df["importance"] = result['importance']
    fold_importance_df["fold"] = fold_nr + 1
    feature_importance_df = pd.concat(
        [feature_importance_df, fold_importance_df], axis=0)

    # we perform predictions by chunks
    initial_idx = 0
    chunk_size = 1000000
    current_pred = np.zeros(len(TEST))
    while initial_idx < TEST.shape[0]:
        final_idx = min(initial_idx + chunk_size, TEST.shape[0])
        idx = range(initial_idx, final_idx)
        current_pred[idx] = clf.predict(
            TEST.iloc[idx][features], num_iteration=result['best_iteration'])
        initial_idx = final_idx
    predictions += current_pred / folds_nr

    score[fold_nr] = metrics.roc_auc_score(TARGET.iloc[val_idx], oof[val_idx])
  

print("prediction completed: ", time.strftime("%b %d %Y %H:%M:%S"))

print("prediction time", time.time() - start)


cv_score = metrics.roc_auc_score(TARGET, oof)

cv_score_printable = "{:<8.5f}".format(cv_score)
print("CV score: {}".format(cv_score_printable))

cv_score_printable = cv_score_printable.replace(".", "")
cv_score_printable = cv_score_printable.strip()


# Feature importance
cols = (feature_importance_df[["feature", "importance"]]
        .groupby("feature")
        .mean()
        .sort_values(by="importance", ascending=False)[:1000].index)

best_features = feature_importance_df.loc[
    feature_importance_df.feature.isin(cols)]

plt.figure(figsize=(14, 25))
sns.barplot(x="importance",
            y="feature",
            data=best_features.sort_values(by="importance",
                                           ascending=False))
plt.title('LightGBM Features (avg over folds)')
plt.tight_layout()
plt.savefig(
    '../img/e{}_lgbm_importances_{}.png'.format(experinment_nr, cv_score_printable))
feature_importance_df.to_csv(
    '../EDA/e{}_lgbm_importances_{}.csv'.format(experinment_nr, cv_score_printable))


# submit predictions

sub_df = pd.read_csv('../input/sample_submission.csv')
sub_df["HasDetections"] = predictions

model_dir = '../output'


model_name = 'submit_e{}_cv{}.csv.gz'.format(
    experinment_nr, cv_score_printable)

fname = os.path.join(model_dir, model_name)
param_string = ', '.join(('{}: {}'.format(k, v) for k, v in PARAM.items()))
message = 'CV: {} DATA: {} LGBM PARAMs: {}'.format(
    cv_score_printable, train_fname, param_string)
competition = 'microsoft-malware-prediction'

sub_df.to_csv(fname, compression='gzip', index=False)
kaggle.api.competition_submit(os.path.abspath(fname), message, competition)

print("script finished: ", time.strftime("%b %d %Y %H:%M:%S"))
