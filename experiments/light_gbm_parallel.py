# coding: utf-8
import warnings
import gc
import pickle
import time
import kaggle
import os


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import lightgbm as lgb
from sklearn.model_selection import KFold

from sklearn import metrics


from multiprocessing import Lock, Process, Queue, current_process

"""
E61
- parallel CV
"""
print("script started: ", time.strftime("%b %d %Y %H:%M:%S"))


experinment_nr = 61
#train_fname = '../input/train-encoded-full-corr-v2.pkl'
#test_fname = '../input/test-encoded-full-corr-v2.pkl'

train_fname = '../input/train_catboost.pkl.gz'
test_fname = '../input/test_catboost.pkl.gz'

TRAIN = pd.read_pickle(train_fname)
print("TRAIN LOADED")


TARGET = pd.read_pickle('../input/target.pkl')


true_numerical_columns = [
    'Census_ProcessorCoreCount',
    'Census_PrimaryDiskTotalCapacity',
    'Census_SystemVolumeTotalCapacity',
    'Census_TotalPhysicalRAM',
    'Census_InternalPrimaryDiagonalDisplaySizeInInches',
    'Census_InternalPrimaryDisplayResolutionHorizontal',
    'Census_InternalPrimaryDisplayResolutionVertical',
    'Census_InternalBatteryNumberOfCharges'
]

categorical_columns = [c for c in TRAIN.columns if c not in true_numerical_columns]



gc.collect()
print("TRAIN PREPARED")
TEST = pd.read_pickle(test_fname)
print("TEST LOADED")

FOLDS_NR = 3

folds = KFold(n_splits=FOLDS_NR, shuffle=True, random_state=15)
oof = np.zeros(len(TRAIN))
features = [c for c in TRAIN.columns if c not in ['MachineIdentifier']]
categorical_columns = [
    c for c in categorical_columns if c not in ['MachineIdentifier']]

predictions = np.zeros(len(TEST))
feature_importance_df = pd.DataFrame()
score = [0 for _ in range(folds.n_splits)]
work_queue = Queue()
done_queue = Queue()
processes = []

PREDICTIONS_CACHE = {}
OOF_CACHE = {}
IMPORTANCE_CACHE = {}

PARAM = {
    'num_threads': 9,
    'num_leaves': 60,
    'min_data_in_leaf': 60,
    "boosting": "gbdt",
    'objective': 'binary',
    "metric": 'auc',
    'max_depth': -1,
    'learning_rate': 0.2,
    "feature_fraction": 0.8,
    "bagging_freq": 1,
    "bagging_fraction": 0.8,
    "bagging_seed": 11,
    "lambda_l1": 0.1,
    "random_state": 133,
    "verbosity": -1
}


def worker(work_queue, done_queue):
    """
    Worker si bere data z fronty, dokud tam nějaká jsou a volá na ně procesní funkci
    """

    try:
        for data in iter(work_queue.get, 'STOP'):

            trn_idx = data['trn_idx']
            val_idx = data['val_idx']
            features = data['features']

            # create dataset for lightgbm
            trn_data = lgb.Dataset(TRAIN.iloc[trn_idx][features],
                                   label=TARGET.iloc[trn_idx],
                                   categorical_feature=categorical_columns
                                   )
            val_data = lgb.Dataset(TRAIN.iloc[val_idx][features],
                                   label=TARGET.iloc[val_idx],
                                   categorical_feature=categorical_columns
                                   )
            # create classifier
            clf = lgb.train(PARAM,
                            trn_data,
                            data['num_round'],
                            valid_sets=[trn_data, val_data],
                            verbose_eval=100,
                            early_stopping_rounds=200)

            # predict
            oof_fo = clf.predict(
                    TRAIN.iloc[val_idx][features], num_iteration=clf.best_iteration)

            print("{} finished training".format(current_process().name))

            # we perform predictions by chunks
            initial_idx = 0
            chunk_size = 1000000
            current_pred = np.zeros(len(TEST))
            while initial_idx < TEST.shape[0]:
                final_idx = min(initial_idx + chunk_size, TEST.shape[0])
                idx = range(initial_idx, final_idx)
                current_pred[idx] = clf.predict(
                    TEST.iloc[idx][features], num_iteration=clf.best_iteration)
                initial_idx = final_idx

            PREDICTIONS_CACHE[fold] = current_pred 
            OOF_CACHE[fold] = oof_fo
            IMPORTANCE_CACHE[fold] = clf.feature_importance(importance_type='gain')

            result = {
                'fold': data['fold'],
                'val_idx': val_idx,
                'trn_idx': trn_idx
            }

            print("{} finished prediction".format(current_process().name))
            print("storing result", result)

            done_queue.put(result)
            print("done queue size", done_queue.qsize())
            print("done queue full", done_queue.full())

    except Exception as e:
        done_queue.put("{} failed with: {}".format(
            current_process().name, e))
    
    print("{} finished".format(current_process().name), time.strftime("%b %d %Y %H:%M:%S"))
    return True


start = time.time()
print("STARTING PARALLEL K-FOLD CV", time.strftime("%b %d %Y %H:%M:%S"))
for fold_, (trn_idx, val_idx) in enumerate(folds.split(TRAIN.values, TARGET.values)):
    print("fold n°{}".format(fold_))

    num_round = 5200

    data = {
        'val_idx': val_idx,
        'trn_idx': trn_idx,
        'fold': fold_,
        'features': features,
        'num_round': 5000
    }

    work_queue.put(data)
    print("queue size", work_queue.qsize())
    print("queue full", work_queue.full())


for w in range(FOLDS_NR):
    # nastartujeme nový process - worker musí dostat frontu jako argument
    p = Process(target=worker, args=(work_queue, done_queue))
    p.start()
    processes.append(p)
    print("started process", w)
    # STOP příkaz musíme přidat pro každý process který jsme vytvořili
    work_queue.put('STOP')
    
for p in processes:
    print(p)
    p.join()
    print('joined', p)



done_queue.put('STOP')

print("training completed: ", time.strftime("%b %d %Y %H:%M:%S"))

print("training time", time.time() - start)

print("starting final prediction: ", time.strftime("%b %d %Y %H:%M:%S"))
start = time.time()

for result in iter(done_queue.get, 'STOP'):
    print(result)
    val_idx = result['val_idx']
    fold_nr = result['fold']

    oof[val_idx] = OOF_CACHE[fold_nr]
    
    fold_importance_df = pd.DataFrame()
    fold_importance_df["feature"] = features
    fold_importance_df["importance"] = IMPORTANCE_CACHE[fold_nr]
    fold_importance_df["fold"] = fold_nr + 1
    feature_importance_df = pd.concat(
        [feature_importance_df, fold_importance_df], axis=0)

    predictions += PREDICTIONS_CACHE[fold_nr] / FOLDS_NR

    score[fold_nr] = metrics.roc_auc_score(TARGET.iloc[val_idx], oof[val_idx])
  
print("prediction merge completed: ", time.strftime("%b %d %Y %H:%M:%S"))

print("prediction time", time.time() - start)


cv_score = metrics.roc_auc_score(TARGET, oof)

cv_score_printable = "{:<8.5f}".format(cv_score)
print("CV score: {}".format(cv_score_printable))

cv_score_printable = cv_score_printable.replace(".", "")
cv_score_printable = cv_score_printable.strip()


# Feature importance
cols = (feature_importance_df[["feature", "importance"]]
        .groupby("feature")
        .mean()
        .sort_values(by="importance", ascending=False)[:1000].index)

best_features = feature_importance_df.loc[
    feature_importance_df.feature.isin(cols)]

plt.figure(figsize=(14, 25))
sns.barplot(x="importance",
            y="feature",
            data=best_features.sort_values(by="importance",
                                           ascending=False))
plt.title('LightGBM Features (avg over folds)')
plt.tight_layout()
plt.savefig(
    '../img/e{}_lgbm_importances_{}.png'.format(experinment_nr, cv_score_printable))
feature_importance_df.to_csv(
    '../EDA/e{}_lgbm_importances_{}.csv'.format(experinment_nr, cv_score_printable))


# submit predictions

sub_df = pd.read_csv('../input/sample_submission.csv')
sub_df["HasDetections"] = predictions

model_dir = '../output'


model_name = 'submit_e{}_cv{}.csv.gz'.format(
    experinment_nr, cv_score_printable)

fname = os.path.join(model_dir, model_name)
param_string = ', '.join(('{}: {}'.format(k, v) for k, v in PARAM.items()))
message = 'CV: {} DATA: {} LGBM PARAMs: {}'.format(
    cv_score_printable, train_fname, param_string)
competition = 'microsoft-malware-prediction'

sub_df.to_csv(fname, compression='gzip', index=False)
kaggle.api.competition_submit(os.path.abspath(fname), message, competition)

print("script finished: ", time.strftime("%b %d %Y %H:%M:%S"))
