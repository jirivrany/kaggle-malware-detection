# coding: utf-8
import numpy as np
import pandas as pd
from datetime import datetime as dt
import os
import lightgbm as lgb
import time
import datetime
from tqdm import tqdm
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold, KFold, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, roc_auc_score
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn import metrics
import gc
import matplotlib.pyplot as plt
import seaborn as sns


train = pd.read_pickle('../input/train.pkl')
print("TRAIN LOADED")
test = pd.read_pickle('../input/test.pkl')
print("TEST LOADED")


experiment_nr = 50
random_state = 42

index_id = (pd.concat([train[['MachineIdentifier',
                              'AvSigVersion',
                              'AppVersion',
                              'EngineVersion',
                              'HasDetections']],
                       test[['MachineIdentifier',
                             'AvSigVersion',
                             'AppVersion',
                             'EngineVersion']]],
                      axis=0, sort=False)
            .reset_index(drop=True)
            .sort_values(['AvSigVersion', 'AppVersion', 'EngineVersion'])
            .reset_index(drop=True))

index_id = pd.merge(index_id, (index_id[['AvSigVersion',
                                         'AppVersion',
                                         'EngineVersion']].drop_duplicates()
                               .reset_index(drop=True)
                               .reset_index()
                               .rename({'index': 'index_id'}, axis=1)),
                    on=['AvSigVersion',
                        'AppVersion',
                        'EngineVersion'], how='left')


train['index_id'] = (index_id[index_id.HasDetections.notnull()]
                     .sort_values(['MachineIdentifier'])
                     .reset_index(drop=True))['index_id']

test['index_id'] = (index_id[index_id.HasDetections.isnull()]
                    .sort_values(['MachineIdentifier'])
                    .reset_index(drop=True))['index_id']

train = train.sort_values(['index_id']).reset_index(drop=True)
y_train = train['HasDetections']

train.drop(['MachineIdentifier', 'HasDetections'], axis=1, inplace=True)


sep = int(len(train) * 0.8)


lgb_params = {
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'metric': 'auc',
    'learning_rate': 0.05,
    'max_depth': 5,
    'num_leaves': 40,
    'sub_feature': 0.9,
    'sub_row': 0.9,
    'bagging_freq': 1,
    'lambda_l1': 0.1,
    'lambda_l2': 0.1,
    'random_state': random_state
}


feature_importance_df = pd.DataFrame()


trn_data = lgb.Dataset(train[:sep],
                       label=y_train[:sep],
                       # categorical_feature=categorical_columns
                       )

val_data = lgb.Dataset(train[sep:],
                       label=y_train[sep:],
                       # categorical_feature=categorical_columns
                       )

classifier_model = lgb.train(lgb_params,
                             trn_data,
                             5200,
                             valid_sets=[trn_data, val_data],
                             verbose_eval=100,
                             early_stopping_rounds=50
                             )

#del train
#del y_train
gc.collect()


predictions = classifier_model.predict(
    train[sep:], num_iteration=classifier_model.best_iteration)
false_positive_rate, recall, thresholds = metrics.roc_curve(
    y_train[sep:], predictions)
score = metrics.auc(false_positive_rate, recall)


fold_importance_df = pd.DataFrame()
fold_importance_df["feature"] = train.columns
fold_importance_df["importance"] = classifier_model.feature_importance(
    importance_type='gain')
feature_importance_df = pd.concat(
    [feature_importance_df, fold_importance_df], axis=0)


cols = (feature_importance_df[["feature", "importance"]]
        .groupby("feature")
        .mean()
        .sort_values(by="importance", ascending=False)[:1000].index)

best_features = feature_importance_df.loc[
    feature_importance_df.feature.isin(cols)]

plt.figure(figsize=(14, 25))
sns.barplot(x="importance",
            y="feature",
            data=best_features.sort_values(by="importance",
                                           ascending=False))
plt.title('LightGBM Features (avg over folds)')
plt.tight_layout()
plt.savefig('../img/e{}_lgbm_importances.png'.format(experiment_nr))

# mean_gain = feature_importances[['gain', 'feature']].groupby('feature').mean()
# feature_importances['mean_gain'] = feature_importances['feature'].map(mean_gain['gain'])
#
# temp = feature_importances.sort_values('mean_gain', ascending=False)
best_features.sort_values(by="importance", ascending=False).groupby("feature").mean().sort_values(
    by="importance", ascending=False)             .to_csv('feature_importances_new.csv', index=True)


del train
del y_train
gc.collect()


# In[ ]:


submission_data = test[['MachineIdentifier']]
predictions = np.zeros(len(test))
test = test.drop('MachineIdentifier', axis=1)
chunk_size = 1000000

predictions = np.zeros(len(test))
initial_idx = 0
while initial_idx < test.shape[0]:
    final_idx = min(initial_idx + chunk_size, test.shape[0])
    idx = range(initial_idx, final_idx)
    predictions[idx] = classifier_model.predict(test.iloc[idx],
                                                num_iteration=classifier_model.best_iteration)
    initial_idx = final_idx


del test


submission_data['HasDetections'] = predictions
filename = 'sub_{}_{:.6f}_{}_data.csv.gz'.format(
    experiment_nr, score, dt.now().strftime('%Y-%m-%d-%H-%M'))
submission_data.to_csv('../output/single_{}'.format(filename),
                       index=False,  compression='gzip')
