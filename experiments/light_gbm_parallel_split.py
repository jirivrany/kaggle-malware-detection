# coding: utf-8
import warnings
import gc
import pickle
import time
import kaggle
import os


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold

from sklearn import metrics


from multiprocessing import Manager, Process, Queue, current_process

"""
E73
- new train / validation split based on time
"""
print("script started: ", time.strftime("%b %d %Y %H:%M:%S"))


experinment_nr = 73
#train_fname = '../input/train-encoded-full-corr-v2.pkl.gz'
#test_fname = '../input/test-encoded-full-corr-v2.pkl.gz'

train_fname = '../input/train_encoded_split.pkl'
test_fname = '../input/test_encoded_split.pkl'
valid_fname = '../input/valid_encoded_split.pkl'

if train_fname.endswith('.gz'):
    TRAIN = pd.read_pickle(train_fname, compression='gzip')
else:    
    TRAIN = pd.read_pickle(train_fname)

print("TRAIN LOADED")

if test_fname.endswith('.gz'):
    TEST = pd.read_pickle(test_fname, compression='gzip')
else:
    TEST = pd.read_pickle(test_fname)
print("TEST LOADED")

if valid_fname.endswith('.gz'):
    VALID = pd.read_pickle(test_fname, compression='gzip')
else:
    VALID = pd.read_pickle(test_fname)
print("TEST LOADED")

TARGET_TRAIN = pd.read_pickle('../input/target_train.pkl')
TARGET_VALID = pd.read_pickle('../input/target_valid.pkl')

FOLDS_NR = 5
JOBS_NR = 5
THREAD_NR = 6

oof = np.zeros(len(TRAIN))
FEATURES = [c for c in TRAIN.columns if c not in ['MachineIdentifier']]

predictions = np.zeros(len(TEST))
feature_importance_df = pd.DataFrame()
score = [0 for _ in range(FOLDS_NR)]
work_queue = Queue()
done_queue = Queue()
processes = []


def worker(work_queue, done_queue, predictions_cache, oof_cache, importance_cache):
    """
    Worker si bere data z fronty, dokud tam nějaká jsou a volá na ně procesní funkci
    """

    try:
        for data in iter(work_queue.get, 'STOP'):
            print("{} GOT DATA".format(current_process().name), data)

            fold = int(data['fold'])

            print("fold", fold, "type", type(fold))

            fractions = [0.7, 0.75, 0.8, 0.85, 0.9]

            param = {
                'num_leaves': 2048,
                'max_bin': 2000,
                'num_threads': THREAD_NR,
                'objective': 'binary',
                'max_depth': -1,
                'learning_rate': 0.05,
                "boosting": "gbdt",
                "feature_fraction": fractions[fold],
                "bagging_freq": 1,
                "bagging_fraction": fractions[fold],
                "bagging_seed": 11,
                "metric": 'auc',
                "lambda_l1": 0.1,
                "random_state": 133 * (fold + 1),
                "verbosity": -1
            }

            

            # create dataset for lightgbm
            trn_data = lgb.Dataset(TRAIN[FEATURES],
                                   label=TARGET_TRAIN
                                   )
            val_data = lgb.Dataset(VALID[FEATURES],
                                   label=TARGET_VALID
                                   )
            # create classifier
            clf = lgb.train(param,
                            trn_data,
                            data['num_round'],
                            valid_sets=[trn_data, val_data],
                            verbose_eval=50,
                            early_stopping_rounds=200)

            # predict
            oof_fo = clf.predict(VALID[FEATURES], num_iteration=clf.best_iteration)

            oof_cache[fold] = oof_fo
            importance_cache[fold] = clf.feature_importance(importance_type='gain')

            print("{} finished training".format(current_process().name))

            # we perform predictions by chunks
            initial_idx = 0
            chunk_size = 1000000
            current_pred = np.zeros(len(TEST))
            while initial_idx < TEST.shape[0]:
                final_idx = min(initial_idx + chunk_size, TEST.shape[0])
                idx = range(initial_idx, final_idx)
                current_pred[idx] = clf.predict(
                    TEST.iloc[idx][FEATURES], num_iteration=clf.best_iteration)
                initial_idx = final_idx

            predictions_cache[fold] = current_pred 
            
            result = fold

            print("{} finished prediction".format(current_process().name))
            print("storing result", result)

            done_queue.put(result)
    
    except Exception as e:
        done_queue.put("{} failed with: {}".format(
            current_process().name, e))
    
    print("{} finished".format(current_process().name), time.strftime("%b %d %Y %H:%M:%S"))
    return True

with Manager() as manager:
    
    predictions_cache = manager.dict()
    oof_cache = manager.dict()
    importance_cache = manager.dict()
   

    start = time.time()
    print("STARTING BAGING + CV", time.strftime("%b %d %Y %H:%M:%S"))
    for fold_ in range(FOLDS_NR):
        print("bag n°{}".format(fold_))

        num_round = 10000

        data = {
            'fold': fold_,
            'num_round': num_round
        }

        work_queue.put(data)
        

    for w in range(JOBS_NR):
        # nastartujeme nový process - worker musí dostat frontu jako argument
        p = Process(target=worker, args=(work_queue, done_queue, predictions_cache, oof_cache, importance_cache))
        p.start()
        processes.append(p)
        print("started process", w)
        # STOP příkaz musíme přidat pro každý process který jsme vytvořili
        work_queue.put('STOP')

        
    for p in processes:
        p.join()
        print('joined', p)



    done_queue.put('STOP')

    print("training completed: ", time.strftime("%b %d %Y %H:%M:%S"))

    print("training time", time.time() - start)

    print("starting final prediction: ", time.strftime("%b %d %Y %H:%M:%S"))
    start = time.time()

    for result in iter(done_queue.get, 'STOP'):
        print("RESULT", result)
        fold_nr = int(result)

    

        
        fold_importance_df = pd.DataFrame()
        fold_importance_df["feature"] = FEATURES
        fold_importance_df["importance"] = importance_cache[fold_nr]
        fold_importance_df["fold"] = fold_nr + 1
        feature_importance_df = pd.concat(
            [feature_importance_df, fold_importance_df], axis=0)

        predictions += predictions_cache[fold_nr] / FOLDS_NR

        valid_predictions += oof_cache[fold_nr] / FOLDS_NR

        score[fold_nr] = metrics.roc_auc_score(TARGET_VALID, oof_cache[fold_nr])
      
    print("prediction merge completed: ", time.strftime("%b %d %Y %H:%M:%S"))

    print("prediction time", time.time() - start)


cv_score = metrics.roc_auc_score(TARGET_VALID, valid_predictions)

cv_score_printable = "{:<8.5f}".format(cv_score)
print("CV score: {}".format(cv_score_printable))

cv_score_printable = cv_score_printable.replace(".", "")
cv_score_printable = cv_score_printable.strip()


# Feature importance
cols = (feature_importance_df[["feature", "importance"]]
        .groupby("feature")
        .mean()
        .sort_values(by="importance", ascending=False)[:1000].index)

best_FEATURES = feature_importance_df.loc[
    feature_importance_df.feature.isin(cols)]

plt.figure(figsize=(14, 25))
sns.barplot(x="importance",
            y="feature",
            data=best_FEATURES.sort_values(by="importance",
                                           ascending=False))
plt.title('LightGBM Features (avg over folds)')
plt.tight_layout()
plt.savefig(
    '../img/e{}_lgbm_importances_{}.png'.format(experinment_nr, cv_score_printable))
feature_importance_df.to_csv(
    '../EDA/importances/e{}_lgbm_importances_{}.csv'.format(experinment_nr, cv_score_printable))


# submit predictions
PARAM = {
    'num_leaves': 2048,
    'max_bin': 2000,
    'num_threads': THREAD_NR,
    'objective': 'binary',
    'max_depth': -1,
    'learning_rate': 0.05,
    "boosting": "gbdt",
    "feature_fraction": [0.7, 0.75, 0.8, 0.85, 0.9],
    "bagging_freq": 1,
    "bagging_fraction": [0.7, 0.75, 0.8, 0.85, 0.9],
    "bagging_seed": 11,
    "metric": 'auc',
    "lambda_l1": 0.1,
    "random_state": 'different for each fold',
    "verbosity": -1
}



sub_df = pd.read_csv('../input/sample_submission.csv')
sub_df["HasDetections"] = predictions

model_dir = '../output'


model_name = 'submit_e{}_cv{}.csv.gz'.format(
    experinment_nr, cv_score_printable)

fname = os.path.join(model_dir, model_name)
param_string = ', '.join(('{}: {}'.format(k, v) for k, v in PARAM.items()))
message = 'CV: {} DATA: {} LGBM PARAMs: {}'.format(
    cv_score_printable, train_fname, param_string)
competition = 'microsoft-malware-prediction'

sub_df.to_csv(fname, compression='gzip', index=False)
kaggle.api.competition_submit(os.path.abspath(fname), message, competition)

fname = os.path.join(model_dir, 'validations', model_name)
sub_df["HasDetections"] = valid_predictions
sub_df.to_csv(fname, compression='gzip', index=False)

print("script finished: ", time.strftime("%b %d %Y %H:%M:%S"))
