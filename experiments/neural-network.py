
# IMPORT LIBRARIES
import pandas as pd
import numpy as np
import math
import os
import gc

import os
os.environ['CUDA_VISIBLE_DEVICES']="1"


from keras import callbacks
from sklearn.metrics import roc_auc_score

from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense, Dropout, BatchNormalization, Activation
from keras.callbacks import LearningRateScheduler
from keras.optimizers import Adam
from keras.utils import multi_gpu_model




# LOAD AND FREQUENCY-ENCODE
FE = ['EngineVersion', 'AppVersion', 'AvSigVersion', 'Census_OSVersion']
# LOAD AND ONE-HOT-ENCODE
OHE = ['RtpStateBitfield', 'IsSxsPassiveMode', 'DefaultBrowsersIdentifier',
       'AVProductStatesIdentifier', 'AVProductsInstalled', 'AVProductsEnabled',
       'CountryIdentifier', 'CityIdentifier',
       'GeoNameIdentifier', 'LocaleEnglishNameIdentifier',
       'Processor', 'OsBuild', 'OsSuite',
       'SmartScreen', 'Census_MDC2FormFactor',
       'Census_OEMNameIdentifier',
       'Census_ProcessorCoreCount',
       'Census_ProcessorModelIdentifier',
       'Census_PrimaryDiskTotalCapacity', 'Census_PrimaryDiskTypeName',
       'Census_HasOpticalDiskDrive',
       'Census_TotalPhysicalRAM', 'Census_ChassisTypeName',
       'Census_InternalPrimaryDiagonalDisplaySizeInInches',
       'Census_InternalPrimaryDisplayResolutionHorizontal',
       'Census_InternalPrimaryDisplayResolutionVertical',
       'Census_PowerPlatformRoleName', 'Census_InternalBatteryType',
       'Census_InternalBatteryNumberOfCharges',
       'Census_OSEdition', 'Census_OSInstallLanguageIdentifier',
       'Census_GenuineStateName', 'Census_ActivationChannel',
       'Census_FirmwareManufacturerIdentifier',
       'Census_IsTouchEnabled', 'Census_IsPenCapable',
       'Census_IsAlwaysOnAlwaysConnectedCapable', 'Wdft_IsGamer',
       'Wdft_RegionIdentifier']

# LOAD ALL AS CATEGORIES
dtypes = {}
for x in FE + OHE:
    dtypes[x] = 'category'
dtypes['MachineIdentifier'] = 'str'
dtypes['HasDetections'] = 'int8'

# LOAD CSV FILE
df_train = pd.read_csv('../input/train.csv',
                       usecols=dtypes.keys(), dtype=dtypes)
print('Loaded', len(df_train), 'rows of TRAIN.CSV!')

## DOWNSAMPLE
#sm = 200000
#df_train = df_train.sample(sm)
#print('Only using', sm, 'rows to train and validate')
#x = gc.collect()


# CHECK FOR NAN
def nan_check(x):
    if isinstance(x, float):
        if math.isnan(x):
            return True
    return False

# FREQUENCY ENCODING


def encode_FE(df, col, verbose=1):
    d = df[col].value_counts(dropna=False)
    n = col + "_FE"
    df[n] = df[col].map(d) / d.max()
    if verbose == 1:
        print('FE encoded', col)
    return [n]

# ONE-HOT-ENCODE ALL CATEGORY VALUES THAT COMPRISE MORE THAN
# "FILTER" PERCENT OF TOTAL DATA AND HAS SIGNIFICANCE GREATER THAN "ZSCORE"


def encode_OHE(df, col, filter, zscore, tar='HasDetections', m=0.5, verbose=1):
    cv = df[col].value_counts(dropna=False)
    cvd = cv.to_dict()
    vals = len(cv)
    th = filter * len(df)
    sd = zscore * 0.5 / math.sqrt(th)
    # print(sd)
    n = []
    ct = 0
    d = {}
    for x in cv.index:
        try:
            if cv[x] < th:
                break
            sd = zscore * 0.5 / math.sqrt(cv[x])
        except:
            if cvd[x] < th:
                break
            sd = zscore * 0.5 / math.sqrt(cvd[x])
        if nan_check(x):
            r = df[df[col].isna()][tar].mean()
        else:
            r = df[df[col] == x][tar].mean()
        if abs(r - m) > sd:
            nm = col + '_BE_' + str(x)
            if nan_check(x):
                df[nm] = (df[col].isna()).astype('int8')
            else:
                df[nm] = (df[col] == x).astype('int8')
            n.append(nm)
            d[x] = 1
        ct += 1
        if (ct + 1) >= vals:
            break
    if verbose == 1:
        print('OHE encoded', col, '- Created', len(d), 'booleans')
    return [n, d]

# ONE-HOT-ENCODING from dictionary


def encode_OHE_test(df, col, dt):
    n = []
    for x in dt:
        n += encode_BE(df, col, x)
    return n

# BOOLEAN ENCODING


def encode_BE(df, col, val):
    n = col + "_BE_" + str(val)
    if nan_check(val):
        df[n] = df[col].isna()
    else:
        df[n] = df[col] == val
    df[n] = df[n].astype('int8')
    return [n]


# In[ ]:


cols = []
dd = []

# ENCODE NEW
for x in FE:
    cols += encode_FE(df_train, x)
for x in OHE:
    tmp = encode_OHE(df_train, x, 0.005, 5)
    cols += tmp[0]
    dd.append(tmp[1])
print('Encoded', len(cols), 'new variables')

# REMOVE OLD
for x in FE + OHE:
    del df_train[x]
print('Removed original', len(FE + OHE), 'variables')
x = gc.collect()

class printAUC(callbacks.Callback):

    def __init__(self, X_train, y_train, model_file_name):
        super(printAUC, self).__init__()
        self.bestAUC = 0
        self.X_train = X_train
        self.y_train = y_train
        self.model_fname = model_file_name

    def on_epoch_end(self, epoch, logs={}):
        pred = self.model.predict(np.array(self.X_train))
        auc = roc_auc_score(self.y_train, pred)
        print("Train AUC: " + str(auc))
        pred = self.model.predict(self.validation_data[0])
        auc = roc_auc_score(self.validation_data[1], pred)
        print("Validation AUC: " + str(auc))
        if (self.bestAUC < auc):
            self.bestAUC = auc
            self.model.save(self.model_fname, overwrite=True)
        return


# In[ ]:



model_save_name = "../models/bestNet.h5"

# SPLIT TRAIN AND VALIDATION SET
X_train, X_val, Y_train, Y_val = train_test_split(
    df_train[cols], df_train['HasDetections'], test_size=0.1)

# BUILD MODEL
model = Sequential()
model.add(Dense(256, input_dim=len(cols)))
model.add(Dropout(100))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dense(256))
model.add(Dropout(0.4))
model.add(BatchNormalization())
model.add(Dense(256))
model.add(Dropout(0.4))
model.add(BatchNormalization())
model.add(Dense(100))
model.add(Dropout(0.4))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dense(1, activation='sigmoid'))

#model = multi_gpu_model(model, gpus=8)
model.compile(optimizer=Adam(lr=0.05),
              loss="binary_crossentropy", metrics=["accuracy"])
annealer = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** x)

print(model.summary())


# TRAIN MODEL
model.fit(X_train, Y_train, batch_size=64, epochs=10, callbacks=[annealer,
                                                                 printAUC(X_train, Y_train, model_save_name)], validation_data=(X_val, Y_val), verbose=2)


# # Predict Test and Submit to Kaggle
# Even after deleting the training data, our network still needs lot of
# our available RAM, we'll need to load in test.csv by chunks and predict
# by chunks. Click 'see code' button to see how this is done.

# In[ ]:


del df_train
del X_train, X_val, Y_train, Y_val
x = gc.collect()

# LOAD BEST SAVED NET
from keras.models import load_model
model = load_model(model_save_name)

pred = np.zeros((7853253, 1))
id = 1
chunksize = 2000000
for df_test in pd.read_csv('../input/test.csv',
                           chunksize=chunksize, usecols=list(dtypes.keys())[0:-1], dtype=dtypes):
    print('Loaded', len(df_test), 'rows of TEST.CSV!')
    # ENCODE TEST
    cols = []
    for x in FE:
        cols += encode_FE(df_test, x, verbose=0)
    for x in range(len(OHE)):
        cols += encode_OHE_test(df_test, OHE[x], dd[x])
    # PREDICT TEST
    end = (id) * chunksize
    if end > 7853253:
        end = 7853253
    pred[(id - 1) * chunksize:end] = model.predict(df_test[cols])
    print('  encoded and predicted part', id)
    id += 1


# In[ ]:


# SUBMIT TO KAGGLE
df_test = pd.read_csv('../input/test.csv', usecols=['MachineIdentifier'])
df_test['HasDetections'] = pred
df_test.to_csv('../output/sub-nn-01.csv', index=False)
