# coding: utf-8
import warnings
import gc
import time
import kaggle
import os

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import lightgbm as lgb
from sklearn.model_selection import KFold


from sklearn.metrics import mean_squared_error
warnings.simplefilter(action='ignore', category=FutureWarning)
from sklearn import metrics

experinment_nr = 28 
train_fname = '../input/train-encoded.pkl'
test_fname = '../input/test-encoded.pkl'

train = pd.read_pickle(train_fname)
print("TRAIN LOADED")


true_numerical_columns = [
    'Census_ProcessorCoreCount',
    'Census_PrimaryDiskTotalCapacity',
    'Census_SystemVolumeTotalCapacity',
    'Census_TotalPhysicalRAM',
    'Census_InternalPrimaryDiagonalDisplaySizeInInches',
    'Census_InternalPrimaryDisplayResolutionHorizontal',
    'Census_InternalPrimaryDisplayResolutionVertical',
    'Census_InternalBatteryNumberOfCharges'
]


# We also list binary variables, since they can be treated as numericals
# by tree methods:
binary_variables = [c for c in train.columns if train[c].nunique() == 2]
# to finally make a census of the categorical variables:
categorical_columns = [c for c in train.columns
                       if (c not in true_numerical_columns) & (c not in binary_variables)]


target = train['HasDetections']
del train['HasDetections']


param = {
    'num_leaves': 40,
    'min_data_in_leaf': 60,
    "boosting": "gbdt",
    'objective': 'binary',
    "metric": 'auc',
    'max_depth': 5,
    'learning_rate': 0.1,
    "feature_fraction": 0.8,
    "bagging_freq": 1,
    "bagging_fraction": 0.8,
    "bagging_seed": 11,
    "lambda_l1": 0.1,
    "random_state": 133,
    "verbosity": -1
}


#max_iter = 3

gc.collect()

print("TRAIN PREPARED")


test = pd.read_pickle(test_fname)
print("TEST LOADED")

explore = [3, 5, 7]

for max_iter in explore:
    gc.collect()

    folds = KFold(n_splits=max_iter, shuffle=True, random_state=15)
    oof = np.zeros(len(train))
    categorical_columns = [
        c for c in categorical_columns if c not in ['MachineIdentifier']]
    features = [c for c in train.columns if c not in ['MachineIdentifier']]
    predictions = np.zeros(len(test))
    start = time.time()
    feature_importance_df = pd.DataFrame()
    start_time = time.time()
    score = [0 for _ in range(folds.n_splits)]

    print("STARTING K-FOLD CV")
    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):
        print("fold nÂ°{}".format(fold_))
        trn_data = lgb.Dataset(train.iloc[trn_idx][features],
                               label=target.iloc[trn_idx],
                               categorical_feature=categorical_columns
                               )
        val_data = lgb.Dataset(train.iloc[val_idx][features],
                               label=target.iloc[val_idx],
                               categorical_feature=categorical_columns
                               )

        num_round = 10000
        clf = lgb.train(param,
                        trn_data,
                        num_round,
                        valid_sets=[trn_data, val_data],
                        verbose_eval=100,
                        early_stopping_rounds=200)

        oof[val_idx] = clf.predict(
            train.iloc[val_idx][features], num_iteration=clf.best_iteration)

        fold_importance_df = pd.DataFrame()
        fold_importance_df["feature"] = features
        fold_importance_df["importance"] = clf.feature_importance(
            importance_type='gain')
        fold_importance_df["fold"] = fold_ + 1
        feature_importance_df = pd.concat(
            [feature_importance_df, fold_importance_df], axis=0)

        # we perform predictions by chunks
        initial_idx = 0
        chunk_size = 1000000
        current_pred = np.zeros(len(test))
        while initial_idx < test.shape[0]:
            final_idx = min(initial_idx + chunk_size, test.shape[0])
            idx = range(initial_idx, final_idx)
            current_pred[idx] = clf.predict(
                test.iloc[idx][features], num_iteration=clf.best_iteration)
            initial_idx = final_idx
        predictions += current_pred / min(folds.n_splits, max_iter)

        print("time elapsed: {:<5.2}s".format((time.time() - start_time) / 3600))
        score[fold_] = metrics.roc_auc_score(target.iloc[val_idx], oof[val_idx])
        if fold_ == max_iter - 1:
            break

    if (folds.n_splits == max_iter):
        cv_score = metrics.roc_auc_score(target, oof)
    else:
        cv_score = sum(score) / max_iter

    cv_score_printable = "{:<8.5f}".format(cv_score)
    print("CV score: {}".format(cv_score_printable))

    cv_score_printable = cv_score_printable.replace(".", "")
    cv_score_printable = cv_score_printable.strip()
    # Feature importance


    cols = (feature_importance_df[["feature", "importance"]]
            .groupby("feature")
            .mean()
            .sort_values(by="importance", ascending=False)[:1000].index)

    best_features = feature_importance_df.loc[
        feature_importance_df.feature.isin(cols)]

    plt.figure(figsize=(14, 25))
    sns.barplot(x="importance",
                y="feature",
                data=best_features.sort_values(by="importance",
                                               ascending=False))
    plt.title('LightGBM Features (avg over folds)')
    plt.tight_layout()
    plt.savefig('../img/lgbm_importances_{}.png'.format(cv_score_printable))
    feature_importance_df.to_csv('../EDA/e{}_lgbm_importances_{}.csv'.format(experinment_nr, cv_score_printable))


    # submit predictions

    sub_df = pd.DataFrame({"MachineIdentifier": test["MachineIdentifier"].values})
    sub_df["HasDetections"] = predictions
    sub_df[:10]


    model_dir = '../output'


    model_name = 'submit_e{}_{}_nl{}_mdl{}_cv{}.csv.gz'.format(experinment_nr, param['boosting'], param[
        'num_leaves'], param['min_data_in_leaf'], cv_score_printable)

    fname = os.path.join(model_dir, model_name)
    param_string = ', '.join(('{}: {}'.format(k, v) for k, v in param.items()))
    message = 'DATA: {} LGBM params: {}'.format(train_fname, param_string)
    competition = 'microsoft-malware-prediction'

    sub_df.to_csv(fname, compression='gzip', index=False)
    kaggle.api.competition_submit(os.path.abspath(fname), message, competition)
